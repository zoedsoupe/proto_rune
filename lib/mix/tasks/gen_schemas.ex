defmodule Mix.Tasks.GenSchemas do
  @moduledoc """
  Generates Elixir code from Lexicon schema files.

  ## Overview

  - **Complex types** (`object`, `record`) become their own modules with a struct and `@type t`.
  - **Primitive or “inline” types** become `@type`s within a single `ProtoRune.Types` module.
  - **Queries/Procedures** also generate minimal modules, mainly containing `@type input` and `@type output`.

  By default, code is written into `lib/proto_rune`, but you can configure via `--output-dir`.
  """

  use Mix.Task

  @shortdoc "Generates Elixir modules/types from Lexicon schemas"

  # ------------------------------------------------------------------
  # Main entry point
  # ------------------------------------------------------------------

  @impl Mix.Task
  def run(args) do
    {opts, _} = parse_args(args)
    path = opts[:path] || raise_missing_path!()
    output_dir = opts[:output_dir] || "lib/proto_rune/lexicons"
    File.mkdir_p!(output_dir)

    files = expand_lexicon_files(path)

    # Load and parse all schemas
    lexicons = load_lexicons!(files)
    defs_map = build_defs_map(lexicons)
    sorted_defs = sort_definitions(defs_map)

    # We track two outputs:
    # 1) A single "ProtoRune.Types" file for all basic types
    # 2) A set of modules (files) for complex definitions
    types_file_path = Path.join("lib/proto_rune", "lexicons.ex")

    # Start with an empty set of lines for the "ProtoRune.Types" file
    base_types_lines = [
      "# File generated by Mix.Tasks.GenSchemas",
      "defmodule ProtoRune.Types do",
      "  @moduledoc \"\"\"",
      "  Central module holding all basic (non-struct) types from Lexicon schemas.",
      "  Generated automatically by `mix gen_schemas`.",
      "  \"\"\""
    ]

    # Keep a context of (1) map of definitions, (2) modules already generated,
    # (3) the lines for the shared base types file, (4) the chosen output_dir.
    context = %{
      defs_map: defs_map,
      generated_modules: MapSet.new(),
      base_types_lines: base_types_lines,
      output_dir: output_dir
    }

    # Generate everything in topological order (or fallback if there's a cycle)
    final_context =
      Enum.reduce(sorted_defs, context, fn {lex_id, name}, ctx ->
        definition = Map.fetch!(defs_map, {lex_id, name})
        generate_code(lex_id, name, definition, ctx)
      end)

    # Finish up the shared Types file
    finalize_types_file(final_context, types_file_path)

    Mix.shell().info("Schemas generated successfully!")
  end

  # ------------------------------------------------------------------
  # Parsing and input
  # ------------------------------------------------------------------

  defp parse_args(args) do
    OptionParser.parse!(args, strict: [path: :string, output_dir: :string])
  end

  defp raise_missing_path! do
    Mix.raise("""
    Missing --path argument. Usage:
        mix gen_schemas --path priv/lexicons/
    """)
  end

  defp expand_lexicon_files(path) do
    if File.dir?(path) do
      Path.wildcard(Path.join(path, "**/*.json"))
    else
      Mix.raise("Invalid path: #{path}")
    end
  end

  defp load_lexicons!(files) do
    files
    |> Enum.map(fn file ->
      case File.read(file) do
        {:ok, contents} ->
          case Jason.decode(contents) do
            {:ok, json} -> Map.put(json, "file_path", file)
            error -> Mix.raise("JSON decode error for #{file}: #{inspect(error)}")
          end

        {:error, reason} ->
          Mix.raise("Cannot read #{file}: #{inspect(reason)}")
      end
    end)
  end

  # ------------------------------------------------------------------
  # Build a definitions map
  # ------------------------------------------------------------------

  defp build_defs_map(lexicons) do
    Enum.reduce(lexicons, %{}, fn lexicon, acc ->
      lex_id = lexicon["id"] || "unknown.id"
      defs = gather_definitions(lexicon)

      Enum.reduce(defs, acc, fn {def_name, def_val}, acc_inner ->
        Map.put(acc_inner, {lex_id, def_name}, def_val)
      end)
    end)
  end

  defp gather_definitions(lexicon) do
    top_level =
      if lexicon["type"] do
        %{"main" => lexicon}
      else
        %{}
      end

    Map.merge(top_level, lexicon["defs"] || %{})
  end

  # ------------------------------------------------------------------
  # Sort definitions by dependency
  # ------------------------------------------------------------------

  defp sort_definitions(defs_map) do
    graph = :digraph.new()

    try do
      # Create vertices
      for {{lex_id, name}, _} <- defs_map do
        :digraph.add_vertex(graph, {lex_id, name})
      end

      # Create edges
      for {{lex_id, name}, definition} <- defs_map do
        refs = collect_refs(lex_id, definition)

        for {r_lex, r_name} <- refs do
          :digraph.add_vertex(graph, {r_lex, r_name})
          # Edge: ref -> this
          :digraph.add_edge(graph, {r_lex, r_name}, {lex_id, name})
        end
      end

      case :digraph_utils.topsort(graph) do
        false ->
          Mix.shell().info("Circular dependency detected. Results may be arbitrary.")
          :digraph.vertices(graph)

        sorted ->
          sorted
      end
    after
      :digraph.delete(graph)
    end
  end

  # ------------------------------------------------------------------
  # Collect references from a definition
  # ------------------------------------------------------------------

  defp collect_refs(current_lex, %{"type" => "ref", "ref" => ref}) do
    [parse_ref(current_lex, ref)]
  end

  defp collect_refs(current_lex, %{"type" => "union", "refs" => refs}) when is_list(refs) do
    Enum.map(refs, &parse_ref(current_lex, &1))
  end

  defp collect_refs(current_lex, %{"properties" => props}) when is_map(props) do
    props
    |> Enum.flat_map(fn {_k, v} -> collect_refs(current_lex, v) end)
  end

  defp collect_refs(current_lex, %{"items" => items}) when is_map(items) do
    collect_refs(current_lex, items)
  end

  # For query/procedure shape
  defp collect_refs(current_lex, %{"input" => inp}) when is_map(inp),
    do: collect_refs(current_lex, inp)

  defp collect_refs(current_lex, %{"output" => out}) when is_map(out),
    do: collect_refs(current_lex, out)

  defp collect_refs(current_lex, %{"parameters" => params}) when is_map(params) do
    Enum.flat_map(params, fn {_k, v} -> collect_refs(current_lex, v) end)
  end

  defp collect_refs(current_lex, %{"schema" => s}) when is_map(s),
    do: collect_refs(current_lex, s)

  defp collect_refs(current_lex, %{"errors" => errs}) when is_list(errs) do
    Enum.flat_map(errs, fn err ->
      if Map.has_key?(err, "schema"),
        do: collect_refs(current_lex, err["schema"]),
        else: []
    end)
  end

  # Anything else
  defp collect_refs(_current_lex, _), do: []

  # Parse "com.example.foo#bar" => { "com.example.foo", "bar" }
  # Or "#bar" => { current_lex, "bar" }
  # Or "com.example.foo" => { "com.example.foo", "main" }
  defp parse_ref(current_lex, ref) do
    cond do
      String.starts_with?(ref, "#") ->
        {current_lex, String.trim_leading(ref, "#")}

      String.contains?(ref, "#") ->
        [nsid, local] = String.split(ref, "#", parts: 2)
        {nsid, local}

      true ->
        {ref, "main"}
    end
  end

  # ------------------------------------------------------------------
  # Main code generation
  # ------------------------------------------------------------------

  defp generate_code(lex_id, name, definition, context) do
    module_id = {lex_id, name}

    if MapSet.member?(context.generated_modules, module_id) do
      # Already generated
      context
    else
      # Mark it generated
      new_generated = MapSet.put(context.generated_modules, module_id)
      updated_ctx = %{context | generated_modules: new_generated}

      # Decide if we generate a struct module or a raw type
      case get_def_type(definition) do
        # record & object => struct-based module
        type when type in ["object", "record"] ->
          do_generate_struct_module(lex_id, name, definition, updated_ctx)

        # query / procedure => minimal module with @type input / output
        type when type in ["query", "procedure"] ->
          do_generate_query_proc_module(lex_id, name, definition, updated_ctx)

        # otherwise => produce a raw type in "ProtoRune.Types"
        _ ->
          new_ctx = produce_raw_type(lex_id, name, definition, updated_ctx)
          new_ctx
      end
    end
  end

  defp do_generate_struct_module(lex_id, name, definition, context) do
    mod_name = build_module_name(lex_id, name)
    file_path = module_to_file_path(mod_name, context.output_dir)

    description = Map.get(definition, "description", "No description.")
    props = Map.get(definition, "properties", %{})
    required = Map.get(definition, "required", [])
    nullable = Map.get(definition, "nullable", [])

    # Build struct fields list (keyword of field_name: nil)
    struct_fields = Enum.map(props, fn {k, _v} -> String.to_atom(k) end)

    # Build @type lines for each field
    typespec_lines =
      Enum.map(props, fn {prop_name, prop_def} ->
        atom_name = String.to_atom(prop_name)
        base_t = map_to_typespec(lex_id, prop_def, context)

        if prop_name in nullable or atom_name in nullable do
          "#{atom_name}: #{base_t} | nil"
        else
          "#{atom_name}: #{base_t}"
        end
      end)

    enforce_keys_clause =
      if required == [],
        do: "",
        else: "@enforce_keys #{inspect(Enum.map(required, &String.to_atom/1))}"

    file_contents = """
    # Generated by Mix.Tasks.GenSchemas
    defmodule #{inspect(mod_name)} do
      @moduledoc \"\"\"
      **#{name}** (object/record)

      #{description}
      \"\"\"

      #{enforce_keys_clause}
      defstruct #{inspect(Enum.map(struct_fields, &{&1, nil}))}

      @type t :: %__MODULE__{
        #{Enum.join(typespec_lines, ",\n  ")}
      }
    end
    """

    File.mkdir_p!(Path.dirname(file_path))
    File.write!(file_path, file_contents)
    context
  end

  defp do_generate_query_proc_module(lex_id, name, definition, context) do
    mod_name = build_module_name(lex_id, name)
    file_path = module_to_file_path(mod_name, context.output_dir)
    description = Map.get(definition, "description", "No description.")

    # If there's input
    input_def = definition["input"]

    input_spec =
      if is_map(input_def),
        do: map_to_typespec(lex_id, input_def, context),
        else: "any()"

    # If there's output
    output_def = definition["output"]

    output_spec =
      if is_map(output_def),
        do: map_to_typespec(lex_id, output_def, context),
        else: "any()"

    file_contents = """
    # Generated by Mix.Tasks.GenSchemas
    defmodule #{inspect(mod_name)} do
      @moduledoc \"\"\"
      **#{name}** (#{definition["type"]})

      #{description}
      \"\"\"

      @type input :: #{input_spec}
      @type output :: #{output_spec}
    end
    """

    File.mkdir_p!(Path.dirname(file_path))
    File.write!(file_path, file_contents)
    context
  end

  # Produces a raw type (enum, tokens, basic "string" with knownValues, etc.) in ProtoRune.Types
  defp produce_raw_type(lex_id, name, definition, context) do
    typedoc_str = Map.get(definition, "description", "No description.")
    # Build the type spec string
    type_string = map_to_typespec(lex_id, definition, context)

    # We'll call it something like app_bsky_actor_defs_profile or the like
    type_alias = raw_type_name(lex_id, name)

    # Append lines to the shared list
    new_lines = [
      "",
      "  @typedoc \"\"\"",
      "  #{name} (#{lex_id})",
      "  #{typedoc_str}",
      "  \"\"\"",
      "  @type #{type_alias} :: #{type_string}"
    ]

    append_type_lines(context, new_lines)
  end

  # ------------------------------------------------------------------
  # Inserting lines into the shared "ProtoRune.Types" file
  # ------------------------------------------------------------------

  defp finalize_types_file(context, types_file_path) do
    all_lines = context.base_types_lines ++ ["end\n"]
    File.mkdir_p!(Path.dirname(types_file_path))
    File.write!(types_file_path, Enum.join(all_lines, "\n"))
  end

  defp append_type_lines(context, new_lines) do
    updated_lines = context.base_types_lines ++ new_lines
    %{context | base_types_lines: updated_lines}
  end

  # ------------------------------------------------------------------
  # Mapping definitions to Elixir type specs
  # ------------------------------------------------------------------

  defp map_to_typespec(current_lex, %{"type" => "ref", "ref" => ref}, context) do
    {r_lex, r_name} = parse_ref(current_lex, ref)
    # Ensure target is generated
    context2 = ensure_generated(r_lex, r_name, context)

    # Now see if the ref is a struct module or a raw type
    ref_def = context2.defs_map[{r_lex, r_name}]

    case get_def_type(ref_def) do
      "object" -> "#{inspect(build_module_name(r_lex, r_name))}.t()"
      "record" -> "#{inspect(build_module_name(r_lex, r_name))}.t()"
      "query" -> "#{inspect(build_module_name(r_lex, r_name))}.output"
      "procedure" -> "#{inspect(build_module_name(r_lex, r_name))}.output"
      _ -> "ProtoRune.Types.#{raw_type_name(r_lex, r_name)}"
    end
  end

  defp map_to_typespec(current_lex, %{"type" => "union", "refs" => refs}, context)
       when is_list(refs) do
    # Union of references
    Enum.map(refs, fn r ->
      {r_lex, r_name} = parse_ref(current_lex, r)
      ctx = ensure_generated(r_lex, r_name, context)
      ref_def = ctx.defs_map[{r_lex, r_name}]

      case get_def_type(ref_def) do
        "object" -> "#{inspect(build_module_name(r_lex, r_name))}.t()"
        "record" -> "#{inspect(build_module_name(r_lex, r_name))}.t()"
        "query" -> "#{inspect(build_module_name(r_lex, r_name))}.output"
        "procedure" -> "#{inspect(build_module_name(r_lex, r_name))}.output"
        _ -> "ProtoRune.Types.#{raw_type_name(r_lex, r_name)}"
      end
    end)
    |> Enum.join(" | ")
  end

  defp map_to_typespec(current_lex, %{"type" => "object"} = defn, context) do
    # Some object definitions are inline without "name".
    name_in_def = Map.get(defn, "name")

    if name_in_def do
      # Generate or ensure we have a struct module for it
      ensure_generated(current_lex, name_in_def, context)
      "#{inspect(build_module_name(current_lex, name_in_def))}.t()"
    else
      # inline object => produce a map type
      props = Map.get(defn, "properties", %{})
      nullable = Map.get(defn, "nullable", [])

      fields =
        Enum.map(props, fn {k, v} ->
          base_t = map_to_typespec(current_lex, v, context)
          is_nullable? = k in nullable
          if is_nullable?, do: "#{k}: #{base_t} | nil", else: "#{k}: #{base_t}"
        end)

      "%{#{Enum.join(fields, ", ")}}"
    end
  end

  defp map_to_typespec(current_lex, %{"type" => "array", "items" => items}, context) do
    item_str = map_to_typespec(current_lex, items, context)
    "list(#{item_str})"
  end

  # Basic scalar types
  defp map_to_typespec(_lex, %{"type" => "null"}, _ctx), do: "nil"
  defp map_to_typespec(_lex, %{"type" => "boolean"}, _ctx), do: "boolean()"
  defp map_to_typespec(_lex, %{"type" => "integer"}, _ctx), do: "integer()"
  defp map_to_typespec(_lex, %{"type" => "float"}, _ctx), do: "float()"
  defp map_to_typespec(_lex, %{"type" => "bytes"}, _ctx), do: "binary()"
  defp map_to_typespec(_lex, %{"type" => "cid-link"}, _ctx), do: "binary()"
  defp map_to_typespec(_lex, %{"type" => "blob"}, _ctx), do: "binary()"
  defp map_to_typespec(_lex, %{"type" => "token"}, _ctx), do: "atom()"
  defp map_to_typespec(_lex, %{"type" => "unknown"}, _ctx), do: "any()"

  # Strings can have enum, const, knownValues
  defp map_to_typespec(_lex, %{"type" => "string"} = defn, _ctx) do
    cond do
      defn["const"] ->
        ":\"#{defn["const"]}\""

      is_list(defn["knownValues"]) ->
        defn["knownValues"]
        |> Enum.map(&":\"#{&1}\"")
        |> Enum.join(" | ")

      is_list(defn["enum"]) ->
        defn["enum"]
        |> Enum.map(&":\"#{&1}\"")
        |> Enum.join(" | ")

      true ->
        "String.t()"
    end
  end

  # Fallback
  defp map_to_typespec(_lex, _defn, _ctx), do: "any()"

  # ------------------------------------------------------------------
  # Ensuring references are generated
  # ------------------------------------------------------------------

  defp ensure_generated(ref_lex, ref_name, context) do
    ref_id = {ref_lex, ref_name}

    if MapSet.member?(context.generated_modules, ref_id) do
      context
    else
      case context.defs_map[ref_id] do
        nil ->
          # It's referencing something that doesn't exist? We'll skip
          context

        ref_def ->
          generate_code(ref_lex, ref_name, ref_def, context)
      end
    end
  end

  # ------------------------------------------------------------------
  # Utilities
  # ------------------------------------------------------------------

  defp get_def_type(%{"type" => type}), do: type
  defp get_def_type(_), do: nil

  # Build a module name from lexicon ID + local name
  # e.g. "app.bsky.feed" + "post" => ProtoRune.App.Bsky.Feed.Post
  defp build_module_name(lex_id, local_name) do
    base_parts = lex_id |> String.split(".") |> Enum.map(&Macro.camelize/1)
    local_parts = local_name |> String.split("#") |> Enum.map(&Macro.camelize/1)
    Module.concat(["ProtoRune" | base_parts ++ local_parts])
  end

  # Example: "com.example.foo#bar" => "com_example_foo_bar"
  defp raw_type_name(lex_id, name) do
    (lex_id <> "_" <> name)
    |> String.replace("#", "_")
    |> String.replace(".", "_")
    |> Macro.underscore()
  end

  # Convert module name to file path
  defp module_to_file_path(mod_name, output_dir) do
    mod_name
    |> Module.split()
    # skip top-level "ProtoRune"
    |> tl()
    |> Enum.map(&Macro.underscore/1)
    |> Path.join()
    |> then(&Path.join(output_dir, &1 <> ".ex"))
  end
end
